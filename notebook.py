# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STi9RSLqE2lyCTfB5uaC5c3DrE7ptyA0

# Import Library

Melakukan import library yang digunakan dalam proyek ini
"""

import pandas as pd
import numpy as np
import gdown
import zipfile
import os

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from google.colab import files

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow import keras
from keras import layers

"""# Data Loading

Menampilkan dataset yang akan digunakan pada proyek ini
"""

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d arashnic/book-recommendation-dataset
!unzip book-recommendation-dataset.zip

"""Menampilkan jumlah data dari masing-masing dataset"""

books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')

print("Jumlah data books: ", len(books['ISBN'].unique()))
print("Jumlah data ratings: ", len(ratings))
print("Jumlah data users: ", len(users['User-ID'].unique()))

"""# Univariate Exploratory Data Analysis

Melakukan proses investigasi awal pada data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

## Books

Dataset books berisi informasi mengenai buku-buku yang tersedia di dalam sistem Book-Crossing. Tujuan awal dari eksplorasi ini adalah untuk memahami karakteristik data buku secara umum.

**Deskripsi Variabel**
* `ISBN` : Nomor identifikasi unik untuk setiap buku.
* `Book-Title` : Judul buku.
* `Book-Author` : Nama penulis buku.
* `Year-Of-Publication` : Tahun terbit buku.
* `Publisher` : Nama penerbit buku.
* `Image-URL-S` : URL gambar buku ukuran kecil.
* `Image-URL-M` : URL gambar buku ukuran sedang.
* `Image-URL-L` : URL gambar buku ukuran besar.

---
Menampilkan info data
"""

books.info()

"""Menampilkan banyak data unik"""

print('Banyak buku: ', len(books['ISBN'].unique()))
print('Banyak judul buku: ', len(books['Book-Title'].unique()))
print('Banyak pengarang: ', len(books['Book-Author'].unique()))
print('Banyak penerbit: ', len(books['Publisher'].unique()))
print('Jumlah tahun terbit: ', len(books['Year-Of-Publication'].unique()))

"""Ditemukan perbedaan antara jumlah ISBN dan judul buku (Book-Title). Oleh karena itu, kode di bawah ini digunakan untuk melakukan pengecekan jumlah kemunculan setiap judul buku:"""

books['Book-Title'].value_counts()

"""Hasil dari kode tersebut menunjukkan bahwa terdapat beberapa buku dengan judul yang sama.

---
Sebagai contoh, pada judul "Selected Poems"
"""

selected_poems_books = books[books['Book-Title'] == 'Selected Poems']
selected_poems_books

"""Pencarian dengan judul yang sama menghasilkan beberapa entri buku yang identik pada judul, tetapi berbeda pada ISBN, penulis, penerbit, atau tahun terbit.

---
Menampilkan isi data dari Year-Of-Publication
"""

print('List tahun terbit: ', books['Year-Of-Publication'].unique())

"""Ketika melakukan eksplorasi terhadap kolom Year-Of-Publication, ditemukan bahwa beberapa entri berisi data yang bukan merupakan tahun, seperti nama penerbit (DK Publishing Inc, Gallimard) atau angka yang tidak logis (misalnya, 0, 1376, 2020, 2050).

---
Mencoba menampilkan isi tahun yang nilainya tidak sesuai itu
"""

books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Setelah dilakukan investigasi lebih lanjut, kasus-kasus seperti ini ternyata terjadi karena pergeseran data antar kolom saat proses input. Untuk menjaga integritas data, tiga entri dengan kesalahan input tersebut dihapus."""

book_to_drop = ['078946697X', '2070426769', '0789466953']
books = books[~books['ISBN'].isin(book_to_drop)]

"""Kemudian untuk nilai-nilai tahun yang tidak realistis (di luar rentang wajar publikasi, seperti 0 atau di atas 2006) diubah menjadi nilai kosong (NaN) dan diisi dengan rata-rata tahun yang valid menggunakan codingan di bawah ini."""

# Nilai 0 tidak valid dan karena kumpulan data ini diterbitkan pada tahun 2004, diasumsikan tahun-tahun setelah 2006 menjadi
# Menetapkan tahun yang tidak valid sebagai NaN
# Convert 'Year-Of-Publication' to numeric, errors='coerce' will replace invalid parsing with NaN
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')
books.loc[(books['Year-Of-Publication'] > 2006) | (books['Year-Of-Publication'] == 0),'Year-Of-Publication'] = np.nan

# mengganti NaN dengan nilai rata-rata yearOfPublication
books['Year-Of-Publication'].fillna(round(books['Year-Of-Publication'].mean()), inplace=True)

"""Setelah tahapan tersebut, kolom tahun terbit telah berisi nilai yang realistis."""

print('List tahun terbit: ', books['Year-Of-Publication'].unique())

"""Kemudian, tipe data pada kolom Year-Of-Publication diubah dari tipe data object menjadi integer agar bisa diproses secara numerik.

---
Menampilkan info data
"""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)

books.info()

"""Menampilkan barplot 10 Author Teratas dengan Buku Terbitan Terbanyak"""

author_counts = books['Book-Author'].value_counts()

top_10_author = author_counts.head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x=top_10_author.index, y=top_10_author.values)
plt.xticks(rotation=90)
plt.xlabel('Author')
plt.ylabel('Number of Books Published')
plt.title('10 Author Teratas dengan Buku Terbitan Terbanyak')
plt.show()

"""Berdasarkan barplot yang menampilkan 10 penulis teratas dengan jumlah buku terbitan terbanyak, terlihat jelas bahwa Agatha Christie mendominasi dengan jumlah publikasi yang signifikan, jauh melampaui penulis lainnya. William Shakespeare menempati posisi kedua, diikuti oleh Stephen King di urutan ketiga. Secara keseluruhan, grafik ini memperlihatkan distribusi jumlah buku yang diterbitkan oleh para penulis terkemuka, dengan penurunan bertahap dari penulis dengan publikasi terbanyak hingga penulis di urutan kesepuluh, yaitu Charles Dickens.

---
Menampilkan barplot 10 Publisher Teratas dengan Buku Terbitan Terbanyak
"""

publisher_counts = books['Publisher'].value_counts()

top_10_publishers = publisher_counts.head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x=top_10_publishers.index, y=top_10_publishers.values)
plt.xticks(rotation=90)
plt.xlabel('Publisher')
plt.ylabel('Number of Books Published')
plt.title('10 Publisher Teratas dengan Buku Terbitan Terbanyak')
plt.show()

"""Barplot ini menyajikan informasi mengenai 10 penerbit teratas berdasarkan jumlah buku yang telah mereka terbitkan. Terlihat bahwa Harlequin menduduki posisi puncak dengan jumlah publikasi yang jauh lebih tinggi dibandingkan penerbit lainnya. Silhouette berada di urutan kedua, diikuti oleh Pocket dan Ballantine Books yang memiliki jumlah publikasi serupa. Secara keseluruhan, grafik ini menggambarkan adanya dominasi yang signifikan dari beberapa penerbit besar dalam industri penerbitan buku, dengan penurunan jumlah publikasi secara bertahap hingga penerbit di urutan kesepuluh, yaitu Warner Books.

---
Menampilkan 5 data teratas
"""

books.head()

"""Menampilkan missing value"""

print(books.isnull().sum())

"""Hanya ada dua kolom yang memiliki missing values, dan jumlahnya sangat kecil sehingga tidak signifikan terhadap keseluruhan dataset.

##  Ratings

**Deskripsi Variabel**
* `User-ID` : ID unik pengguna.
* `ISBN` : Nomor ISBN buku yang diberi rating.
* `Book-Rating` : Skor rating yang diberikan pengguna (rentang 0–10).

Dataset `ratings` mencatat penilaian yang diberikan oleh pengguna terhadap buku. Dataset ini menjadi sangat penting karena akan digunakan untuk analisis preferensi pengguna dan sistem rekomendasi.

---
Menampilkan info data
"""

ratings.info()

"""Menampilkan data unik dari masing-masing variabel"""

print('Banyak user rating buku: ', len(ratings['User-ID'].unique()))
print('Banyak buku dirating: ', len(ratings['ISBN'].unique()))
print('Banyak skala rating: ', len(ratings['Book-Rating'].unique()))

"""Menampilkan skala rating yang unik (0-10)"""

print('Skala rating yang diberikan: ', ratings['Book-Rating'].unique())

"""Rating diberikan dalam skala **0 hingga 10**, dengan total 11 nilai unik. Berikut adalah interpretasi awal:

* **Rating 0** kemungkinan besar berarti tidak ada rating eksplisit yang diberikan oleh pengguna. Ini dikenal sebagai *implicit rating* atau bisa jadi kesalahan input.
* Rating 1–10 menunjukkan penilaian eksplisit, dengan nilai lebih tinggi menandakan tingkat kesukaan yang lebih besar.

---
Mentotalkan rating berdasarkan skalanya
"""

ratings["Book-Rating"].value_counts()

"""Barplot dari pembagian ratings buku"""

plt.figure(figsize=(8, 6))
sns.countplot(x='Book-Rating', data=ratings)
plt.xlabel('Book Rating')
plt.ylabel('Number of Ratings')
plt.title('Pembagian Pemeringkatan Buku')
plt.show()

"""Barplot ini memperlihatkan distribusi peringkat buku, dengan jelas menunjukkan bahwa mayoritas besar peringkat terkonsentrasi pada nilai 0. Hal ini mengindikasikan bahwa terdapat sejumlah besar buku yang belum atau tidak mendapatkan peringkat. Setelah nilai 0, jumlah peringkat secara umum meningkat seiring dengan kenaikan nilai peringkat, mencapai puncaknya pada peringkat 8, kemudian sedikit menurun pada peringkat 9 dan 10. Distribusi ini menyiratkan adanya polarisasi dalam pemberian peringkat, di mana banyak buku tidak dinilai sama sekali, sementara di antara buku yang dinilai, terdapat kecenderungan pemberian peringkat yang lebih tinggi.

---
Menampilkan 5 data teratas dataset
"""

ratings.head()

"""Menampilkan deskripsi dari dataset rating"""

ratings.describe()

"""Nilai tengah (median) berada di 0, mengindikasikan bahwa sebagian besar rating tidak eksplisit (banyak 0). Namun, nilai kuartil atas (75%) adalah 8, yang memperlihatkan bahwa ketika rating diberikan, nilainya cenderung tinggi.

---
Melakukan pengecekan missing value
"""

print(ratings.isnull().sum())

"""## Users

Dataset `users` mencatat informasi dasar tentang pengguna, seperti ID, lokasi, dan umur.

**Deskripsi Variabel**
* `User-ID` : ID unik pengguna.
* `Location` : Lokasi tempat tinggal pengguna.
* `Age` : Usia pengguna.

Menampilkan info dataset
"""

users.info()

"""Menampilkan dari variabel secara unik"""

print('Banyak user: ', len(users['User-ID'].unique()))
print('Jumlah persebaran lokasi user: ', len(users['Location'].unique()))
print('Banyak umur user: ', len(users['Age'].unique()))

"""Mencari isi data age yang unik"""

print('List umur user: ', users['Age'].unique())

"""Kolom `Age` memiliki nilai-nilai yang tidak masuk akal seperti `0`, `231`, dan `244`. Maka dilakukan pembersihan dengan cara:

* Umur di bawah 5 tahun atau di atas 90 tahun dianggap tidak valid dan diubah menjadi `NaN`
* Nilai `NaN` kemudian diisi dengan rata-rata umur pengguna yang valid
* Tipe data diubah ke `integer` agar lebih konsisten
"""

# Umur di bawah 5 dan di atas 90 tidak masuk akal, maka menggantinya dengan NaN
users.loc[(users.Age > 90) | (users.Age < 5), 'Age'] = np.nan

# Replacing NaNs with mean
users.Age = users.Age.fillna(users.Age.mean())

# Setting the data type as int
users.Age = users.Age.astype(np.int32)

"""Menampilkan boxenplot distribusi usia pengguna"""

# Menggunakan seaborn untuk membuat boxenplot yang menampilkan distribusi usia pengguna.
plt.figure(figsize=(10, 7))
sns.boxenplot(users.Age, color='teal')
plt.title('Distribusi Umur Pengguna')
plt.show()

"""Boxplot ini menyajikan distribusi usia pengguna. Terlihat bahwa sebagian besar data usia terkumpul di sekitar rentang usia 25 hingga 45 tahun, yang ditunjukkan oleh kotak utama. Garis hitam di dalam kotak merepresentasikan median usia. Ekor atau *whisker* pada kedua sisi kotak menunjukkan sebaran data usia di luar kuartil bawah dan atas. Titik di atas *whisker* atas mengindikasikan adanya nilai *outlier*, yaitu satu pengguna dengan usia yang jauh lebih tinggi dibandingkan dengan mayoritas pengguna lainnya.

---
Menampilkan 5 data user teratas
"""

users.head()

"""Menampilkan describe dari data"""

users.describe()

"""Melakukan pencarian missing value"""

print(users.isnull().sum())

"""Tidak ada missing value

# Data Preprocessing

Proses menyiapkan data mentah menjadi format yang lebih siap untuk analisis atau model machine learning

Menampilkan data unik dari ISBN pada dataset buku
"""

print('Banyak buku: ', len(books['ISBN'].unique()))

"""Mengambil 15.000 baris data secara acak dari DataFrame books dengan random_state=5"""

# Mengambil 15.000 baris data secara acak dari DataFrame books dengan random_state=5
reduced_books = books.sample(n=15000, random_state=5)

reduced_books

"""Mengecek apakah data memiliki duplikat"""

unique_isbn_list = reduced_books['ISBN'].unique().tolist()
len(unique_isbn_list)

"""Membuat dataframe baru bernama reduced_ratings yang berisi baris-baris dari dataframe ratings di mana nilai dalam kolom 'ISBN' terdapat dalam list unique_isbn_list. Dengan kata lain, kode ini memfilter dataframe ratings untuk hanya menyertakan rating buku-buku yang ISBN-nya ada dalam daftar ISBN unik."""

reduced_ratings = ratings[ratings['ISBN'].isin(unique_isbn_list)]
reduced_ratings

"""Menghapus tiga kolom, yaitu 'Image-URL-S', 'Image-URL-M', dan 'Image-URL-L', dari dataframe reduced_books. Ini kemungkinan dilakukan untuk menyederhanakan dataframe atau karena kolom-kolom tersebut tidak relevan untuk analisis atau pemodelan yang akan dilakukan selanjutnya."""

reduced_books = reduced_books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])

"""Menggabungkan dataframe rating dengan book berdasarkan nilai ISBN"""

# Menggabungkan dataframe rating dengan book berdasarkan nilai ISBN
booksrate = pd.merge(reduced_ratings, reduced_books, on='ISBN', how='left')
booksrate

"""Menghitung jumlah rating kemudian menggabungkannya berdasarkan ISBN"""

# Menghitung jumlah rating kemudian menggabungkannya berdasarkan ISBN
booksrate.groupby('ISBN').sum()

"""# Data Preparation

Melakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan

---

Setelah dilakukan proses EDA, ditemukan bahwa masih terdapat beberapa nilai kosong (*missing values*) dalam gabungan dataset `booksrate`. Berikut ini adalah jumlah missing value dari masing-masing kolom:
"""

# Cek missing value dengan fungsi isnull()
booksrate.isnull().sum()

# Membersihkan missing value dengan fungsi dropna()
booksrate_clean = booksrate.dropna()

"""Fungsi ini secara otomatis menghapus baris yang memiliki nilai kosong pada salah satu kolom. Karena hanya ada satu baris yang terpengaruh, maka tidak akan berdampak signifikan terhadap keseluruhan dataset. Dengan menghapus missing value maka dapat menghindari error pada saat melakukan analisis atau pemodelan dan memastikan integritas data tetap terjaga tanpa baris yang tidak lengkap.

---
Mengecek kembali missing value dengan fungsi isnull
"""

# Mengecek kembali missing value dengan fungsi isnull()
booksrate_clean.isnull().sum()

"""Menampilkan dataset"""

booksrate_clean

"""Setelah membersihkan *missing value*, langkah selanjutnya adalah memastikan tidak ada data duplikat dalam data buku. Duplikat yang dimaksud di sini adalah data dengan ISBN yang sama muncul lebih dari sekali."""

# Membuat variabel preparation yang berisi dataframe booksrate_clean kemudian membuang data duplikat pada variabel preparation
preparation = booksrate_clean.drop_duplicates('ISBN')
preparation

"""Kolom `ISBN` (International Standard Book Number) merupakan pengenal unik untuk setiap buku. Dengan menghapus data duplikat berdasarkan kolom ini, kita menjamin bahwa tidak ada entri buku yang redundan dalam data yang akan diproses lebih lanjut. Dengan langkah ini, maka dapat menghindari bias data karena satu buku yang sama muncul lebih dari sekali dan menjaga akurasi dalam sistem rekomendasi yang berbasis konten (content-based filtering), karena sistem akan mengenali setiap buku hanya satu kali.

---
Setelah data dibersihkan dari duplikat, kolom-kolom penting seperti `ISBN`, `Book-Title`, `Book-Author`, `Year-Of-Publication`, dan `Publisher` kemudian dikonversi ke dalam bentuk list.

"""

# Mengonversi data series ISBN menjadi dalam bentuk list
book_isbn = preparation['ISBN'].tolist()

# Mengonversi data series ‘Book-Title’ menjadi dalam bentuk list
book_title = preparation['Book-Title'].tolist()

# Mengonversi data series ‘Book-Author’ menjadi dalam bentuk list
book_author = preparation['Book-Author'].tolist()

# Mengonversi data series ‘Year-Of-Publication’ menjadi dalam bentuk list
book_year = preparation['Year-Of-Publication'].tolist()

# Mengonversi data series ‘Publisher’ menjadi dalam bentuk list
book_publisher = preparation['Publisher'].tolist()

print(len(book_isbn))
print(len(book_title))
print(len(book_author))
print(len(book_year))
print(len(book_publisher))

"""Semua list ini memiliki panjang yang sama yaitu 14.930, menandakan bahwa tidak ada data yang hilang dalam proses konversi.Dengan melakukan langkah ini, maka dapat mengubah data menjadi bentuk list memudahkan manipulasi data pada tahap pembuatan *content-based recommender* dan format list lebih fleksibel untuk digunakan dalam pembuatan *dictionary*, *TF-IDF vectorizer*, atau pembuatan fitur metadata gabungan.

# Model Development dengan Content Based Filtering

Langkah awal adalah menyusun *dataframe* baru bernama `books_new`.
"""

# Membuat dictionary untuk data ‘book_isbn’, ‘book_title’, ‘book_author’, dan 'book_publisher'
books_new = pd.DataFrame({
    'isbn': book_isbn,
    'title': book_title,
    'author': book_author,
    'year': book_year,
    'publisher': book_publisher
})
books_new

"""Proses ini melibatkan konversi informasi buku dari format *series* (kemungkinan dari hasil pembacaan *dataset* sebelumnya) menjadi format *list*. *List* ini kemudian digunakan untuk membuat kolom-kolom dalam *dataframe* `books_new`, yang terdiri dari 'isbn', 'book_title', 'book_author', 'year_of_publication', dan 'publisher'. Tujuan dari pembentukan *dataframe* ini adalah untuk memudahkan manipulasi dan transformasi data ke dalam format yang sesuai untuk pemodelan *content-based filtering*.

"""

data = books_new
data.sample(5)

"""## TF-IDF Vectorizer"""

# Gabungkan author dan publisher menjadi satu kolom string
data['combined'] = data['author'].fillna('') + ' ' + data['publisher'].fillna('')

# Inisialisasi TF-IDF
tf = TfidfVectorizer()

# Fit dan transform ke kolom gabungan
tfidf_matrix = tf.fit_transform(data['combined'])

# Melihat fitur yang dihasilkan
features = tf.get_feature_names_out()

# print(tfidf_matrix.shape)
tf.get_feature_names_out()

"""**Penggabungan Fitur Teks**

Untuk merepresentasikan konten setiap buku, informasi dari kolom 'book_author' dan 'publisher' digabungkan menjadi satu kolom teks baru bernama 'combined'.

Langkah ini dilakukan dengan mengisi nilai yang hilang (jika ada) pada kedua kolom tersebut dengan string kosong ('') dan kemudian menggabungkannya dengan spasi di antara keduanya. Kolom 'combined' ini akan menjadi dasar untuk perhitungan kemiripan konten antar buku.

**Ekstraksi Fitur dengan TF-IDF**

Sebuah objek TfidfVectorizer dibuat. Vectorizer ini akan mengubah teks menjadi matriks representasi numerik, di mana setiap kata dalam korpus akan menjadi sebuah fitur. Bobot TF-IDF diberikan kepada setiap kata dalam setiap dokumen (dalam hal ini, setiap entri di kolom 'combined'), yang mencerminkan seberapa penting kata tersebut dalam dokumen relatif terhadap seluruh korpus.

Metode fit_transform() dari objek TfidfVectorizer dipanggil dengan kolom 'combined' sebagai input. Proses fit akan mempelajari kosakata dari seluruh teks dalam kolom 'combined', dan proses transform akan mengubah setiap entri teks menjadi vektor TF-IDF berdasarkan kosakata yang telah dipelajari. Hasilnya adalah matriks tfidf_matrix.

tf.get_feature_names_out() digunakan untuk mendapatkan daftar semua fitur (kata unik) yang telah diekstrak oleh TfidfVectorizer.
"""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['combined'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""tfidf_matrix.shape memberikan dimensi dari matriks TF-IDF. Jumlah baris sesuai dengan jumlah buku dalam dataset, dan jumlah kolom sesuai dengan jumlah fitur unik yang ditemukan dalam kolom 'combined'."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""`tfidf_matrix.todense()` mengubah matriks *sparse* TF-IDF menjadi matriks padat. Meskipun representasi *sparse* lebih efisien untuk penyimpanan dan perhitungan dengan data teks berdimensi tinggi, representasi padat mungkin lebih mudah dipahami dalam beberapa kasus."""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan author, publisher
# Baris diisi dengan nama buku

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.title
).sample(22, axis=1).sample(10, axis=0)

"""Sebuah dataframe dibuat dari matriks padat TF-IDF. Kolom-kolom dataframe ini diberi nama sesuai dengan fitur-fitur yang diekstrak (kata-kata dari 'author' dan 'publisher'), dan indeksnya adalah judul buku dari kolom 'title' (kemungkinan kolom 'book_title' telah diubah namanya menjadi 'title' pada tahap sebelumnya). Metode .sample() digunakan untuk menampilkan sebagian kecil dari dataframe ini, memudahkan visualisasi bobot TF-IDF untuk beberapa buku dan fitur secara acak.

## Cosine Similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Fungsi cosine_similarity() dari library sklearn.metrics.pairwise digunakan untuk menghitung kemiripan cosine antara semua pasangan vektor TF-IDF dalam tfidf_matrix. Cosine similarity adalah ukuran kemiripan antara dua vektor dalam ruang multidimensi dan sering digunakan untuk mengukur kemiripan dokumen teks. Nilainya berkisar antara -1 (tidak mirip) hingga 1 (sangat mirip)."""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Hasil dari cosine_similarity() adalah matriks numpy yang kemudian diubah menjadi dataframe cosine_sim_df. Indeks dan kolom dari dataframe ini diatur menjadi judul buku, sehingga setiap sel (i, j) dalam dataframe berisi nilai cosine similarity antara buku i dan buku j.

cosine_sim_df.shape menunjukkan dimensi dari matriks cosine similarity, yang akan berbentuk persegi dengan ukuran jumlah buku dikali jumlah buku. Metode .sample() kembali digunakan untuk menampilkan sebagian kecil dari matriks cosine similarity, yang menunjukkan tingkat kemiripan antara beberapa pasangan buku secara acak.

## Mendapatkan Rekomendasi

Mendefinisikan fungsi book_recommendations yang menerima judul buku, matriks kemiripan, dataframe informasi buku, dan jumlah rekomendasi yang diinginkan sebagai input. Fungsi ini mencari buku-buku yang paling mirip dengan judul yang diberikan berdasarkan matriks kemiripan dan mengembalikan dataframe berisi k buku rekomendasi teratas beserta informasi judul, penulis, dan penerbitnya.
"""

def book_recommendations(title, similarity_data=cosine_sim_df, items=data[['title', 'author', 'publisher']], k=5):
  index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

  # Mengambil data dengan similarity terbesar dari index yang ada
  closest = similarity_data.columns[index[-1:-(k+2):-1]]

  # Drop title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
  closest = closest.drop(title, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(k)

"""Mencari dan menampilkan baris dalam dataframe data di mana nilai pada kolom 'title' sama dengan string 'Harry Potter and the Prisoner of Azkaban (Book 3)'. Tujuannya adalah untuk melihat informasi lengkap mengenai buku tersebut dalam dataset."""

data[data['title'] == 'Harry Potter and the Prisoner of Azkaban (Book 3)']

"""Memanggil fungsi book_recommendations dengan judul buku 'Harry Potter and the Prisoner of Azkaban (Book 3)' sebagai input. Tujuannya adalah untuk mendapatkan daftar 5 buku rekomendasi teratas yang paling mirip dengan buku tersebut berdasarkan perhitungan kemiripan yang telah dilakukan sebelumnya."""

book_recommendations('Harry Potter and the Prisoner of Azkaban (Book 3)')

"""# Model Development dengan Collaborative Filtering

## Data Understanding
"""

# Membaca dataset
df = reduced_ratings
df

"""Langkah awal dalam persiapan data untuk collaborative filtering adalah menggunakan dataframe yang disebut reduced_ratings. Ini mengindikasikan bahwa mungkin telah dilakukan proses pengurangan atau pemilihan sebagian data rating dari dataset awal. Tujuannya bisa untuk mengurangi kompleksitas komputasi atau fokus pada interaksi pengguna dan item yang lebih relevan.

## Data Preparation
"""

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""df['User-ID'].unique().tolist() menghasilkan list yang berisi nilai-nilai unik dari kolom 'User-ID'. Ini memastikan bahwa setiap pengguna dan setiap buku hanya direpresentasikan satu kali dalam list.

Dictionary ini memetakan setiap nilai unik (ID pengguna) ke sebuah indeks integer yang berurutan. Misalnya, pengguna dengan ID '22' mungkin dipetakan ke indeks 0, pengguna dengan ID '53' ke indeks 1, dan seterusnya.

Ini adalah dictionary kebalikan dari yang sebelumnya, memetakan indeks integer kembali ke nilai asli (ID pengguna atau ISBN). Ini berguna untuk interpretasi hasil model.
"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_isbn = df['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_isbn)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_isbn)}

"""df['ISBN'].unique().tolist() menghasilkan list yang berisi nilai-nilai unik dari kolom 'ISBN'. Ini memastikan bahwa setiap pengguna dan setiap buku hanya direpresentasikan satu kali dalam list.

Dictionary ini memetakan setiap nilai unik (ISBN) ke sebuah indeks integer yang berurutan. Misalnya, pengguna dengan ID '22' mungkin dipetakan ke indeks 0, pengguna dengan ID '53' ke indeks 1, dan seterusnya.

Ini adalah dictionary kebalikan dari yang sebelumnya, memetakan indeks integer kembali ke nilai asli (ID pengguna atau ISBN). Ini berguna untuk interpretasi hasil model.
"""

pd.options.mode.chained_assignment = None

# Mapping User-ID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe book
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""Metode .map() digunakan untuk menerapkan dictionary encoding ke kolom 'User-ID' dan 'ISBN' dalam dataframe df. Ini akan membuat dua kolom baru, 'user' dan 'book', yang berisi indeks integer yang sesuai untuk setiap interaksi (rating)."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_book = len(book_to_book_encoded)
print(num_book)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""Setelah proses encoding, jumlah pengguna unik (num_users) dan jumlah buku unik (num_book) dihitung menggunakan panjang dari dictionary encoding. Informasi ini penting untuk menentukan dimensi dari embedding layer dalam model neural network untuk collaborative filtering.

Kolom 'Book-Rating' diubah menjadi tipe data float32. Ini umum dilakukan untuk nilai rating karena model machine learning seringkali bekerja dengan bilangan floating-point.

Nilai minimum (min_rating) dan maksimum (max_rating) dari kolom 'Book-Rating' diidentifikasi. Informasi ini akan digunakan untuk normalisasi nilai rating ke dalam rentang yang lebih kecil (biasanya 0 hingga 1).

## Membagi Data untuk Training dan Validasi
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Dataframe df diacak menggunakan df.sample(frac=1, random_state=42). frac=1 berarti semua baris akan dikembalikan, tetapi dalam urutan acak. random_state=42 digunakan untuk memastikan bahwa pengacakan akan menghasilkan urutan yang sama setiap kali kode dijalankan, yang penting untuk reproducibility."""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""- Fitur (x): Kolom 'user' dan 'book' (yang berisi indeks encoding) dipilih sebagai fitur (x). Pasangan indeks pengguna dan buku ini akan menjadi input untuk model collaborative filtering. Metode .values digunakan untuk mengubah dataframe menjadi array numpy.
- Label (y): Kolom 'Book-Rating' digunakan sebagai label (y). Nilai rating dinormalisasi ke dalam rentang 0 hingga 1 menggunakan formula: (x - min_rating) / (max_rating - min_rating). Normalisasi ini membantu model untuk belajar dengan lebih stabil dan efisien.

Dataset dibagi menjadi set pelatihan (80%) dan validasi (20%). train_indices dihitung untuk menentukan titik pemisahan. Kemudian, fitur (x) dan label (y) dibagi menjadi x_train, x_val, y_train, dan y_val. Set pelatihan akan digunakan untuk melatih model, sedangkan set validasi akan digunakan untuk mengevaluasi kinerja model selama pelatihan dan membantu dalam tuning hyperparameter.

## Proses Training

Mendefinisikan kelas RecommenderNet, sebuah model neural network untuk rekomendasi. Model ini menggunakan embedding layers untuk merepresentasikan pengguna dan buku dalam ruang vektor laten, serta bias untuk menangkap preferensi individual dan popularitas buku, kemudian menggabungkannya untuk memprediksi rating melalui fungsi sigmoid.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Menginisialisasi model RecommenderNet dengan jumlah pengguna, jumlah buku, dan ukuran embedding sebesar 50, lalu mengompilasinya dengan fungsi loss BinaryCrossentropy, optimizer Adam dengan learning rate 0.001, dan metrik evaluasi Root Mean Squared Error."""

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Memulai proses pelatihan model menggunakan data latih (x_train, y_train) dengan ukuran batch 16 selama 20 epoch, sambil mengevaluasi kinerja model pada data validasi (x_val, y_val) di setiap epoch."""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 20,
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik

Menghasilkan grafik yang memvisualisasikan perubahan nilai Root Mean Squared Error (RMSE) selama proses pelatihan (train) dan evaluasi pada data validasi (test) di setiap epoch.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi Buku

Mengambil sampel seorang pengguna secara acak, mengidentifikasi buku-buku yang belum pernah dibaca oleh pengguna tersebut, dan kemudian menyiapkan data dalam format yang sesuai untuk diprediksi oleh model collaborative filtering.
"""

book_df = books_new

# Mengambil sample user
user_id = reduced_ratings['User-ID'].sample(5).iloc[0]
book_readed_by_user = reduced_ratings[reduced_ratings['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user['ISBN'].values)]['isbn']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_readed = [[book_to_book_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

"""Menggunakan model yang telah dilatih untuk memprediksi rating untuk buku-buku yang belum dibaca oleh pengguna sampel, mengambil 10 buku dengan prediksi rating tertinggi sebagai rekomendasi, dan kemudian mencetak daftar buku yang diberi rating tinggi oleh pengguna tersebut serta 10 rekomendasi teratas."""

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_readed[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_readed_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.title, ':', row.author, '-', row.publisher)

print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.title, ':', row.author, '-', row.publisher)

"""# Evaluation

Tahapan untuk mengevaluasi hasil dari kedua model

## Content Based Filtering

Mengevaluasi model Content-Based Filtering dengan menghitung Precision, Recall, dan F1-Score. Kode ini menetapkan threshold kemiripan, membuat ground truth biner berdasarkan threshold, membandingkan dengan prediksi biner dari matriks cosine similarity, dan mencetak metrik evaluasi.
"""

from sklearn.metrics import precision_recall_fscore_support

# Tentukan threshold kemiripan
threshold = 0.6

# Buat ground truth: 1 jika kemiripan >= threshold, 0 jika tidak
ground_truth = np.where(cosine_sim >= threshold, 1, 0)

# # Tampilkan sebagian ground truth sebagai heatmap untuk visualisasi
# ground_truth_df = pd.DataFrame(ground_truth, index=data['title'], columns=data['title'])
# sample_gt = ground_truth_df.sample(15, axis=0).sample(15, axis=1)

# Evaluasi berdasarkan sample ukuran tertentu
sample_size = 10000  # atau min(len(data), 10000)
cosine_sim_sample = cosine_sim[:sample_size, :sample_size]
ground_truth_sample = ground_truth[:sample_size, :sample_size]

# Flatten semua nilai kemiripan dan ground truth ke bentuk 1D
cosine_sim_flat = cosine_sim_sample.flatten()
ground_truth_flat = ground_truth_sample.flatten()

# Buat prediksi biner berdasarkan threshold
predictions = (cosine_sim_flat >= threshold).astype(int)

# Hitung precision, recall, dan f1-score
precision, recall, f1, _ = precision_recall_fscore_support(
    ground_truth_flat, predictions, average='binary', zero_division=1
)

print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-score:  {f1:.4f}")

"""## Collaborative Filtering

Menghasilkan grafik garis yang menunjukkan nilai Root Mean Squared Error (RMSE) pada data pelatihan dan validasi di setiap epoch selama proses pelatihan model Collaborative Filtering, memungkinkan visualisasi kinerja model dan potensi overfitting.
"""

import matplotlib.pyplot as plt

# Misalnya ini hasil RMSE dari history model
rmse_train = history.history['root_mean_squared_error']
rmse_val = history.history['val_root_mean_squared_error']
epochs = range(1, len(rmse_train) + 1)

plt.figure(figsize=(10, 5))
plt.plot(epochs, rmse_train, marker='o')
plt.plot(epochs, rmse_val, marker='s')
plt.title('Model Metrics')
plt.ylabel('Root Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

plt.grid(True)
plt.tight_layout()
plt.show()